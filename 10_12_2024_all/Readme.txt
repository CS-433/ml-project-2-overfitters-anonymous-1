the two jupiter notebook are just "test" codes. You can use them individually if you want, but they are not needed to run everything.

Each .py codes should be (in principle ) usable individually. 
Make sure to write the correct paths. Also sometimes it may not be cpu compatible. It should be handeled correctly, but It has never been tried on a cpu so it is likely some lines should be changed for cpu use. Anyway if needed they are very small simple fixes.


# classes definitions #####################################################

UNet.py : the class UNet defines the code architecture. A model is a UNet object. 
ImagesDataset.py : the class ImagesDataset allows to handle the dataset more easilly.


# runable files ##########################################################

main.py : just click and run with the desired parameters. It deletes existing augmented data, create new ones, train the model, saves it and show some results.  
It runs firts augment_data.py, then train.py, then show_results.py

augment_data.py : augments the data from an original image and mask folder to a new file, then copy the original images to this new file as well.
-> it augments the data. 

train.py : loads the data ( hopfully the augmented ones ), then normalize them, then train and saves the model in a .pth file at the best mean epoch loss. Other infos are also saved like the mean and std of the trained images etc. 
WILL BE SOON IMPROVED : need to ad validation set and save model on the best validation loss instead of best train loss. 
BE CAREFULL WITH THE NAME YOU GIVE : DO NOT OVERWRITE EXISTING MODEL 

show_results.py : selects 3 random images in the test folder, and show the image, the mask generated by the model, and the superposition of the two.


# the model #############################################################

UNet_4lev_Dice_norm_augmV2_79epochs.pth : 
A UNet with 4 levels, 
Loss is Dice loss,
data are normalied and augmented ( V2 was the 2nd version of augmenting using an older version of augment_data, the current version of augmentation in augment_data is also different from V2. you can call the current V3 if you want.),
model was saved at epoch 79.

This model was saved on an older version of train, so not all the features are saved. If you fail loading some features lke 'Learning_rate', it is basically because they do not exist since they where not stored. However their values are the one by default in the current code, so you can find them again.
